{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento facial e classificação de objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificação de faces e classificação de objetos.\n",
    "\n",
    "Alguns recursos e códigos foram adaptados deste [repositório](https://github.com/udacity/CVND_Exercises/) do curso de Visão Computacional da Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versão local deste notebook, sem o sufixo ```-collab```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requerimentos\n",
    "\n",
    "### 1.1 Bibliotecas\n",
    "\n",
    "Todas as bibliotecas já estão instaladas no Google Colab.\n",
    "\n",
    "* OpenCV>=3.4.3\n",
    "* Pillow>= 7.0.0\n",
    "* Pytorch>=1.4.0\n",
    "* Numpy>=1.18.1\n",
    "* Keras >= 2.3.1\n",
    "* Tensorflow >= 2.2.0\n",
    "* Gdown >= 3.6.4\n",
    "\n",
    "### 1.2 Arquivos\n",
    "\n",
    "Baixe o repositório do GitHub utilizando o comando abaixo. Em caso de atualização, utilize o comando para apagar o diretório antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fiap-ml-visao-computacional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michelpf/fiap-ml-visao-computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora posicionar o diretório do repositório para a aula respectiva. Nesse caso envie o comando de mudança de diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fiap-ml-visao-computacional/aula-5-machine-learning-aplicado/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#Exibição na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir, sep\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imutils import paths, resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classificação de Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer um estudo de benchmarking entre os 3 classificadores: Eingenfaces, Fihserfaces e LBPH.\n",
    "Para entender melhor os pontos positivos de cada um deles, foi utilizado o [dataset de faces da FEI](https://fei.edu.br/~cet/facedatabase.html) de imagnes originais, sem nenhuma modificação. Ao todo são 4 arquivos anexados que possuem 14 imagens de 200 pessoas.\n",
    "Neste estudo vamos utilizar somente a parte 1, que possui 50 sujeitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibição das imagens\n",
    "img1 = cv2.imread(\"faces-fei/1-01.jpg\")\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.imread(\"faces-fei/2-02.jpg\")\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "img3 = cv2.imread(\"faces-fei/3-03.jpg\")\n",
    "img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "img4 = cv2.imread(\"faces-fei/4-04.jpg\")\n",
    "img4 = cv2.cvtColor(img4, cv2.COLOR_BGR2RGB)\n",
    "img5 = cv2.imread(\"faces-fei/5-05.jpg\")\n",
    "img5 = cv2.cvtColor(img5, cv2.COLOR_BGR2RGB)\n",
    "img6 = cv2.imread(\"faces-fei/6-06.jpg\")\n",
    "img6 = cv2.cvtColor(img6, cv2.COLOR_BGR2RGB)\n",
    "img7 = cv2.imread(\"faces-fei/7-07.jpg\")\n",
    "img7 = cv2.cvtColor(img7, cv2.COLOR_BGR2RGB)\n",
    "img8 = cv2.imread(\"faces-fei/8-08.jpg\")\n",
    "img8 = cv2.cvtColor(img8, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "\n",
    "plt.subplot(241)\n",
    "plt.imshow(img1)\n",
    "plt.subplot(242)\n",
    "plt.imshow(img2)\n",
    "plt.subplot(243)\n",
    "plt.imshow(img3)\n",
    "plt.subplot(244)\n",
    "plt.imshow(img4)\n",
    "plt.subplot(245)\n",
    "plt.imshow(img5)\n",
    "plt.subplot(246)\n",
    "plt.imshow(img6)\n",
    "plt.subplot(247)\n",
    "plt.imshow(img7)\n",
    "plt.subplot(248)\n",
    "plt.imshow(img8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Identificação e segmentação de região de interesse\n",
    "\n",
    "As imagens que iremos utilizar estão sem nenhum tratamento. Nosso objetivo aqui é ter um recorte somente do rosto de cada uma das pessoas, removendo o fundo e detalhes do vestuário.\n",
    "\n",
    "Iremos aplicar o classificador de cascatada de Haar e extrair somente as faces de cada imagem, padronizar o tamanho e converter para escala de cinza, que é como os classificadores utilizados trabalham com as imagens. As faces extraídas deverão ser armazenadas na pasta ```treino``` e ```teste```, separando 70% para o treinamento do modelo e 30% para a validação.\n",
    "\n",
    "> Importante: o classificador em cascada de Haar poderá detectar eventualmente falsos positivos, neste caso será fácil eliminar estes casos limitando a área mínima de cada uma delas. Após ensaios cheguei ao valor mínimo de área de rosto de **35000** pixels quadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def extrator_face_bgr(imagem):\n",
    "    \n",
    "    imagem_gray = cv2.cvtColor(imagem,cv2.COLOR_BGR2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    roi_list = []\n",
    "    roi_area_list = []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = h*w\n",
    "        roi_area_list.append(area)\n",
    "        roi_list.append(roi)\n",
    "\n",
    "    max_area = 0\n",
    "    max_area_id = 0\n",
    "    \n",
    "    for idx, area in enumerate(roi_area_list):\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_area_id = idx\n",
    "    \n",
    "    if max_area < 35000:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    return roi_list[max_area_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A padronização da imagem é necessária pois no treinamento todas elas precisam estar no mesmo tamanho. Iremos aplicar o tamanho ```200x200 pixels``` e também iremos converter a imagem para escala de cinza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padronizar_imagem(imagem):\n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    imagem_gray = cv2.resize(imagem_gray, (200, 200), interpolation=cv2.INTER_LANCZOS4)\n",
    "    return imagem_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pequeno ensaio de como as imagens serão transformadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"faces-fei/1-03.jpg\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_RGB2BGR)\n",
    "roi = extrator_face_bgr(imagem)\n",
    "roi = cv2.cvtColor(roi, cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = padronizar_imagem(roi)\n",
    "plt.imshow(roi, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Separando dados de treinamento e teste\n",
    "\n",
    "Cada sujeito (ou face) possui 14 imagens, destas pode existir algumas poses que o classificador não é capaz de identificar, como de lado. Desta forma, vamos trabalhar com imagens de índice até 8 no treino e acima para teste. Na prática, teremos 7 imagens para treino e 3 imagens para teste, alcançando o valor recomendável de validação de 30% do total de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando exemplos de arquivos previamente coletados\n",
    "faces_path = \"faces-fei/\"\n",
    "faces_path_treino = \"faces-fei/treino/\"\n",
    "faces_path_teste = \"faces-fei/teste/\"\n",
    "\n",
    "lista_arquivos = [f for f in listdir(faces_path) if isfile(join(faces_path, f))]\n",
    "\n",
    "contador = 0\n",
    "\n",
    "for arquivo in tqdm(lista_arquivos):\n",
    "    imagem = cv2.imread(faces_path + arquivo)\n",
    "    face = extrator_face_bgr(imagem)\n",
    "   \n",
    "    if face is not None:\n",
    "        subject_num = int(arquivo.split(\".\")[0].split(\"-\")[1])\n",
    "        \n",
    "        if subject_num < 9:\n",
    "            cv2.imwrite(faces_path_treino + arquivo, face)\n",
    "        else:\n",
    "            cv2.imwrite(faces_path_teste + arquivo, face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparação para o treinamento dos modelos\n",
    "\n",
    "Com as imagens separadas, iremos reunir e organizá-las. Em ```lista_imagens_treino``` vamos armazenar todas as imagnes de treino. Na mesma sequência, armazenaremos a identificação do sujeito em ```lista_sujeitos_treino```. Cada sujeito tem uma identificação numérica única, por exemplo o arquivo ```1-03.jpg``` representa a terceira imagem do sujeito ```1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_arquivos_treino = [f for f in listdir(faces_path_treino) if isfile(join(faces_path_treino, f))]\n",
    "\n",
    "lista_sujeitos_treino = []\n",
    "lista_imagens_treino = []\n",
    "\n",
    "for arquivo in tqdm(lista_arquivos_treino):\n",
    "    imagem = cv2.imread(faces_path_treino + arquivo)\n",
    "    imagem = padronizar_imagem(imagem)\n",
    "    lista_imagens_treino.append(imagem)\n",
    "    sujeito = int(arquivo.split(\"-\")[0])\n",
    "    lista_sujeitos_treino.append(sujeito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total de imagens envolvidas no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_sujeitos_treino), len(lista_imagens_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo processo para as imagens de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_arquivos_teste = [f for f in listdir(faces_path_teste) if isfile(join(faces_path_teste, f))]\n",
    "\n",
    "lista_sujeitos_teste = []\n",
    "lista_imagens_teste = []\n",
    "\n",
    "for arquivo in tqdm(lista_arquivos_teste):\n",
    "    imagem = cv2.imread(faces_path_teste + arquivo)\n",
    "    imagem = padronizar_imagem(imagem)\n",
    "    lista_imagens_teste.append(imagem)\n",
    "    sujeito = int(arquivo.split(\"-\")[0])\n",
    "    lista_sujeitos_teste.append(sujeito)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total de imagens envolvidas no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_sujeitos_teste), len(lista_imagens_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a conversão dos rótulos que repressentam a identificação de cada sujeito. O formato ```array``` é requisito dos classificadores para treinarem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_sujeitos_treino = np.asarray(lista_sujeitos_treino, dtype=np.int32)\n",
    "lista_sujeitos_teste = np.asarray(lista_sujeitos_teste, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Treinamento e validação do modelo Eingenfaces\n",
    "\n",
    "Treinamento e testes para validar a precisão do modelo Eingenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_eingenfaces = cv2.face.EigenFaceRecognizer_create()\n",
    "modelo_eingenfaces.train(lista_imagens_treino, lista_sujeitos_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação dos testes de validação, que é a execução do modelo contro as imagens reservadas para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eingenfaces = []\n",
    "\n",
    "for item in tqdm(lista_imagens_teste):\n",
    "    y_pred_eingenfaces.append(modelo_eingenfaces.predict(item)[0])\n",
    "    \n",
    "acuracia_eingenfaces = accuracy_score(lista_sujeitos_teste, y_pred_eingenfaces)\n",
    "acuracia_eingenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Treinamento e validação do modelo Fisherfaces\n",
    "\n",
    "Treinamento e testes para validar a precisão do modelo Fisherfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lda = cv2.face.FisherFaceRecognizer_create()\n",
    "modelo_lda.train(lista_imagens_treino, lista_sujeitos_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lda = []\n",
    "\n",
    "for item in tqdm(lista_imagens_teste):\n",
    "    y_pred_lda.append(modelo_lda.predict(item)[0])\n",
    "    \n",
    "acuracia_lda = accuracy_score(lista_sujeitos_teste, y_pred_lda)\n",
    "acuracia_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Treinamento e validação do modelo LBPH\n",
    "\n",
    "Treinamento e testes para validar a precisão do modelo LBPH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lbph = cv2.face.LBPHFaceRecognizer_create()\n",
    "modelo_lbph.train(lista_imagens_treino, lista_sujeitos_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lbph = []\n",
    "\n",
    "for item in tqdm(lista_imagens_teste):\n",
    "    y_pred_lbph.append(modelo_lbph.predict(item)[0])\n",
    "    \n",
    "acuracia_lbph = accuracy_score(lista_sujeitos_teste, y_pred_lbph)\n",
    "acuracia_lbph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daddo que as imagens não seguem um padrão de captura, ou seja, estão com poses diferentes uma das outras e levando em consideração que em cada sujeito havia uma pose com baixa luminosidade, pudemos verificar que o classificador Eingenfaces não conseguiu obter precisão de 45%. Enquanto a abordagem por LDA, do classificador Fisherfaces, obteve 63%. Por outro lado, o classificador LBPH foi o mais robusto dentre os 3, alcançado 89% de precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O teste a seguir é para verificarmos que além da identificação do sujeito, temos o valor da distância de similaridade. Com ela podemos também deduzir um limite máximo para determinar se a identificação é indeterminada.\n",
    "\n",
    "Nos classificadores Eigenfaces e Fisherfaces, valores de distância até 35 e 45 são considerados bons. Ao passo que no classificador LBPH os valores de similaridades utiilizados estão entre os valores entre 40 e 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"faces-fei/teste/2-09.jpg\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "roi = extrator_face_bgr(imagem)\n",
    "roi_padronizado = padronizar_imagem(roi)\n",
    "\n",
    "predicao = modelo_lbph.predict(roi_padronizado)\n",
    "predicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"Sujeito: 02, Predição: sujeito \" + str(predicao[0]) + \" distância \" + str(predicao[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep learning aplicado a OCR\n",
    "\n",
    "Para criar um modelo de Deep Learning para reconhecimento de caracteres, vamos utilizar a base conhecida gerada de um sistema de captcha que foi utilizado nos sistemas do Tribunal Regional do Trabalho de São Paulo.\n",
    "\n",
    "Neste exemplo, vamos utilizar somente os caracteres que aparecem na maior parte dos desafios de captcha, que na coleta foram de 33 letras e números.\n",
    "\n",
    "A arquitetura e alguns componentes foram adaptados deste [artigo](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d) de Orhan Gazi Yalçın."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identificando as classes\n",
    "\n",
    "Como um gerador de captchas nem sempre explora todo o alfabeto, vamos identificar exatamente quais as letras são utilizadas para listar todas as possíveis classes deste problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_imagens_treino = \"captcha/imagens/\"\n",
    "lista_arquivos_classes = [f for f in listdir(pasta_imagens_treino) if isdir(join(pasta_imagens_treino, f))]\n",
    "\n",
    "lista_classes = list(set(lista_arquivos_classes))\n",
    "print(lista_classes)\n",
    "print(len(lista_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enquadramento de imagem\n",
    "\n",
    "Vamos deixar uma borda de segurança entre as letras para evitar classificações indevidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimensionar_borda(imagem, comprimento, altura):\n",
    "    \n",
    "    # Obtendo as dimensões da imagem\n",
    "    (h, w) = imagem.shape[:2]\n",
    "\n",
    "    # Vamos deixar as imagens quadradas, logo se o comprimento for maior que a altura\n",
    "    # O resize orginal do OpenCV sempre trabalha com altura e comprimento\n",
    "    # A função resize do imutils, dado comprimento ou altura, ajusta o outro parâmetro \n",
    "    # para crescer mantendo o aspecto de razão\n",
    "    if w > h:\n",
    "        imagem = resize(imagem, width=comprimento)\n",
    "    else:\n",
    "        imagem = resize(imagem, height=altura)\n",
    "\n",
    "    # Ajustando a borda\n",
    "    padW = int((comprimento - imagem.shape[1]) / 2)\n",
    "    padH = int((altura - imagem.shape[0]) / 2)\n",
    "\n",
    "    imagem = cv2.copyMakeBorder(imagem, padH, padH, padW, padW, cv2.BORDER_CONSTANT, value=[255,255,255])\n",
    "    imagem = cv2.resize(imagem, (comprimento, altura), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de imagem com tamanho despadronizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"captcha/imagens/i/000001_3ibaz.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(imagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a padronização mas **sem** margem na borda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_sem_borda = cv2.resize(imagem, (20, 20), interpolation=cv2.INTER_LANCZOS4)\n",
    "plt.imshow(imagem_sem_borda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a padronização mas **com** margem na borda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_padronizada = redimensionar_borda(imagem, 20, 20)\n",
    "plt.imshow(imagem_padronizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Treinamento\n",
    "\n",
    "Colecionando imagens para treinamento e realizando pequenos ajustes para posterior uso na biblioteca de deep-learning do Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "dados_imagem = []\n",
    "\n",
    "pasta_imagens_treino = \"captcha/imagens/\"\n",
    "lista_imagens_arquivos = paths.list_images(pasta_imagens_treino)\n",
    "\n",
    "for imagem_caminho in tqdm(lista_imagens_arquivos):\n",
    "    # Obtendo imagem e convertendo para escala de cinza\n",
    "    imagem = cv2.imread(imagem_caminho)\n",
    "    imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    imagem = redimensionar_borda(imagem, 20, 20)\n",
    "\n",
    "    # Adicionando uma terceira dimensão (Canal Normalizado) conforme especificação do Keras\n",
    "    imagem = np.expand_dims(imagem, axis=2)\n",
    "\n",
    "    # Obtendo a caractere pelo nome do diretório\n",
    "    classe = imagem_caminho.split(sep)[-2]\n",
    "\n",
    "    dados_imagem.append(imagem)\n",
    "    classes.append(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao todo temos as seguintes quantidades de exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes), len(dados_imagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também simplificar a informação de escala de cinza. Utilizaremos a forma normalizada, dividindo todos os valores por 255. Desta forma um pixel 100% branco seria 1, e outro 100% preto seria 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_imagem = np.array(dados_imagem, dtype=\"float\") / 255\n",
    "classes = np.array(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a divisão de treinamento e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, y_train, y_test) = train_test_split(dados_imagem, classes, test_size=0.3, random_state=0)\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Keras trabalha com uma forma diferente dos dados. Ao invés de utilizar as 3 dimensões, precisaremos de mais uma dimensão para incluir as imagens que farão parte dos treinamentos e testes, obtendo **Número de Imagens, Comprimento, Largura, Canal Normalizado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato de dados da API Keras\", x_train[0].shape)\n",
    "print('Imagens de treino (x) 20 x 20:', x_train.shape)\n",
    "print('Quantidade de imagens de treino', x_train.shape[0])\n",
    "print('Quantidade de imagens de treino', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir a entrada dos dados, neste caso precisa ser exatamente da mesma forma que as imagens forem treinadas. Isso é portante pois a rede neural estará preparada para inferir somente imagens com este tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_entrada = (20, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluindo codificação *one-hot*, ou seja, um conjunto de dados que está associado as classes, logo um dos 33 caracteres será codificado com bit 1 de acordo com sua posição na lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer().fit(y_train)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de uma amostra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lista_classes)\n",
    "print([lista_classes[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vamos constuir um modelo simples, do zero. Como as imagens são bem simples, diversas arquiteturas funcionam. Quando lidamos com objetos mais complexos, é bem comum optarmos por arquiteturas abertas como por exemplo:\n",
    "\n",
    "* [VGG](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) de Oxford\n",
    "* [ResNet](https://arxiv.org/abs/1512.03385) da Microsoft\n",
    "* [Inception](https://github.com/google/inception) do Google\n",
    "* [Xception](https://arxiv.org/abs/1610.02357) do Google\n",
    "\n",
    "Depois de avaliar estas arquiteturas, é possível adapta-las para classificar imagens específicas, isso se dá alterando as últimas camadas. É o que chamamos também de **Transfer Learning**.\n",
    "\n",
    "Neste [link](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) você pode encontrar mais sobre outras arquiteturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_classes = len(lista_classes)\n",
    "numero_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Arquitetura\n",
    "\n",
    "Partimos de um modelo simples que na maioria das vezes resolve problemas de OCR como esse. Como foi citado, identificações mais complexas utilizamos outras aboragens ou evolução de uma arquitetura inicial como esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Este componente, se trata de um filtro ou uma camada convulacional. Ela será responsável por \n",
    "# colocar uma janela de kernel (5x5), navegar pela imagem e extrair a soma dos pixels de cada janela\n",
    "# o passo para mover a janela, chamado Stride, por padrão é de um pixel\n",
    "model.add(Conv2D(20, kernel_size=(5,5), padding=\"same\", input_shape=shape_entrada, activation=\"relu\"))\n",
    "\n",
    "# A camada de Pooling (ou MaxPooling2D) tem o papel de reduzir a dimensionalidade. Neste caso, a partir \n",
    "# da etapa anterior, será dividia em grupos de 2 x 2 pixels e será obtida o maior valor deles\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "# Esta etapa conhecida como \"achatamento\" é onde abrimos os dados organizados em tabelas (ou matrizes) \n",
    "# para uma única linha\n",
    "model.add(Flatten())\n",
    "\n",
    "# A camada densa (ou Dense) conectará cada elemento da camada anterior e passará para a próxima\n",
    "# camada com as classes existentes\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# O Dropout é um ruído gerado para evitar overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# A camada final, determinará qual classe escolher. Por tal razão ela possui a ativação Softmax, que retorna \n",
    "# a probabilidade por classe\n",
    "model.add(Dense(numero_classes, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para revisão de funções de ativação, em particular [Relu](https://matheusfacure.github.io/2017/07/12/activ-func/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=10, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os gráficos a seguir mostram convergência de acurácia para os dados de treinamento e validação.\n",
    "Note que o valor do erro, diferentemente da acurácia, não é expressada em porcentagem, portanto erro < 1 é um ótimo valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para deixar no formato do Seaborn os gráficos do Pyplot\n",
    "sns.set()\n",
    "\n",
    "# Exibindo dados de Acurácia/Precisão\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Exibindo dados de Perda\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo para uso posterior. Mesmo imagens pequenas como essas levam vários minutos para treinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo no formato HDf5\n",
    "model.save(\"modelos/model_captcha.h5\")\n",
    "\n",
    "# Arquitetura das camadas em JSSON e pesos treinados em HDF5\n",
    "model.save_weights(\"pesos/weights_captcha.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez salvo o modelo, nesta etapa é só carregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo no formato HDf5\n",
    "model = load_model(\"modelos/model_captcha.h5\")  \n",
    "model.load_weights(\"pesos/weights_captcha.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Testes de validação\n",
    "Vamos inferir algumas imagens para verificar visualmente como o classificador está se comportando.\n",
    "Para isso definimos uma função para normalizar uma imagem do captcha, para extrair os ruídos e posteriomente cada uma das suas letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagem_normalizada(caminho_imagem):\n",
    "\n",
    "    imagem = cv2.imread(caminho_imagem)\n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    imagem_suavizada = cv2.GaussianBlur(imagem_gray, (5, 5), 0)\n",
    "    _, imagem_limiarizada =  cv2.threshold(imagem_suavizada, 148, 255, cv2.THRESH_BINARY_INV)\n",
    "    _, imagem_limiarizada =  cv2.threshold(imagem_limiarizada, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    imagem_erodida = cv2.erode(imagem_limiarizada, kernel, iterations = 2)\n",
    "\n",
    "    return imagem_erodida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Conversão para escala de cinza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "imagem_caminho = \"captcha/anotados/d85iq.png\"\n",
    "\n",
    "imagem_original = cv2.imread(imagem_caminho, cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(imagem_original, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Suavização para preparação de limiarização\n",
    "\n",
    "Esta operação visa remover os ruídos da imagem, como as linhas transversais e pequenos pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(imagem_caminho)\n",
    "imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "imagem_suavizada = cv2.GaussianBlur(imagem_gray, (5, 5), 0)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_suavizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Limiarização\n",
    "\n",
    "A limirização remove todos os ruídos baseado num valor de limiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, imagem_limiarizada =  cv2.threshold(imagem_suavizada, 148, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_limiarizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Inversão da imagem\n",
    "\n",
    "Para se adequar as imagens de treinamento e para que fique mais nítido a visualização das letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, imagem_limiarizada =  cv2.threshold(imagem_limiarizada, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_limiarizada, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Erosão\n",
    "\n",
    "Como a imagem está invertida, aplicamos uma erosão para intensificar as linhas e melhorar a nitidez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((2, 2), np.uint8)\n",
    "imagem_erodida = cv2.erode(imagem_limiarizada, kernel, iterations = 2)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_erodida, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execução da função. Neste caso não fizemos a inversão da imagem pois as letras foram treinadas com o fundo branco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_norm = imagem_normalizada(imagem_caminho)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_norm, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Identificação de contornos\n",
    "\n",
    "Com a imagem com as letras bem definidas, iremos aplicar o método Canny para extrair os contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_borda = cv2.Canny(imagem_norm, 30, 200)\n",
    "contornos, _ = cv2.findContours(imagem_borda, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_borda, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_borda_contornos = imagem_borda.copy()\n",
    "imagem_borda_contornos = cv2.cvtColor(imagem_borda_contornos, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "cv2.drawContours(imagem_borda_contornos, contornos, -1, (0,255,0), 1)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_borda_contornos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contornos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 Extração das letras\n",
    "\n",
    "A função a seguir, analisará os contornos identificados e fará um filtro baseado no tamanho do contorno. Em algumas ocasiões é possível ter contornos identificados em pequenos ruídos que ainda passam pelo processo, mas como eles são pequenos são facilmente identificados e removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_letras(imagem):\n",
    "    \n",
    "    contornos_letras = []\n",
    "    \n",
    "    imagem_borda = cv2.Canny(imagem, 30, 200)\n",
    "    contornos, _ = cv2.findContours(imagem_borda, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contorno in contornos:\n",
    "        (x, y, w, h) = cv2.boundingRect(contorno)\n",
    "\n",
    "        area = int(w) * int(h)\n",
    "        \n",
    "        if area <250:\n",
    "            continue\n",
    "        \n",
    "        contornos_letras.append((x, y, w, h))\n",
    "    \n",
    "    print(\"Identificado \" + str(len(contornos_letras)) + \" contornos válidos.\")\n",
    "    \n",
    "    # Se detectar mais do que 5 letras, detecção inválida\n",
    "    if len(contornos_letras) < 5 :\n",
    "        return False\n",
    "    \n",
    "    contornos_letras = sorted(contornos_letras, key=lambda x: x[0])\n",
    "    \n",
    "    lista_imagem_letras = []\n",
    "    \n",
    "    for retangulo_letra in contornos_letras:\n",
    "        x, y, w, h = retangulo_letra\n",
    "        imagem_letra = imagem[y - 10:y + h + 30, x - 1:x + w + 1]\n",
    "        lista_imagem_letras.append(imagem_letra)\n",
    "        \n",
    "    return lista_imagem_letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O retorno da função é a lista de regiões de interesse das letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_letras = obter_letras(imagem_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13 Validação com imagem completa\n",
    "\n",
    "A função a seguir, dada uma imagem, vai padronizá-la e inferir letra a letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_predicao(imagem):\n",
    "    \n",
    "    imagem_norm = redimensionar_borda(imagem, 20, 20)\n",
    "    prediction = model.predict(imagem_norm.reshape(1, 20, 20, 1))\n",
    "    label = lb.inverse_transform(prediction)[0]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(151)\n",
    "plt.title(obter_predicao(imagem_letras[0]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[0], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.title(obter_predicao(imagem_letras[1]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[1], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.title(obter_predicao(imagem_letras[2]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[2], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.title(obter_predicao(imagem_letras[3]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[3], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.title(obter_predicao(imagem_letras[4]), fontdict={'fontsize': 20})\n",
    "plt.imshow(imagem_letras[4], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer learning para reconhecimento de imagens\n",
    "\n",
    "Técnica de transfer learning aplicado a deep learnig para classificação de imagens, utilizando classificados com pesos já treinados disponibilizados no Keras.\n",
    "\n",
    "Foram utilizadas imagens com tamanho 100 x 100 pixels, 1409 imagens por classe para o treinamento e 472 imagens por classe para validação.\n",
    "\n",
    "### 4.1 Geradores de imagens\n",
    "\n",
    "Os geradores utilizados foram aplicados para converter o tamanho adequado do modelo utilizado (224 x 224 pixels) como também para criar novos exemplos a partir das imagens no que chamamos de _data augmentation_, por meio de perturbações da imagem baseado em recorte (```shear```), zoom e orientação horizontal (```horizontal_flip```).\n",
    "\n",
    "Conjunto de dados utilizado foi [este](https://www.kaggle.com/moltean/fruits), disponível no Kaggle.\n",
    "\n",
    "*Adaptado deste [artigo](https://medium.freecodecamp.org/keras-vs-pytorch-avp-transfer-learning-c8b852c31f02), de Patryk Miziula*\n",
    "\n",
    "Com a técnica de _data augmentation_ foram geradas 1212 imagens por classe. Ao todo, o número de imagens subiu de 794 para 2424."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range=10,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"imagens-frutas/train\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    color_mode='rgb',\n",
    "    target_size=(224,224))\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    \"imagens-frutas/validation\",\n",
    "    shuffle=False,\n",
    "    class_mode=\"binary\",\n",
    "    color_mode='rgb',\n",
    "    target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exibindo as classes identificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Construindo a rede neural baseado em modelo pré-treinado\n",
    "\n",
    "O Keras já possui classes especializadas para os seguintes modelos de deep-learning treinados com o conjunto de dados [ImageNet](http://www.image-net.org/):\n",
    "  \n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "* DenseNet\n",
    "* NASNet\n",
    "* MobileNetV2\n",
    "\n",
    "Mais detalhes, veja na [documentação do Keras](https://keras.io/applications/).\n",
    "\n",
    "_O Keras se encarrega de baixar o modelo automaticamente, não é preciso baixar separadamente._\n",
    "\n",
    "Note que o parâmetro ```include_top=False``` configura o modelo para não utilizar a camada densa original, pois será substituída pelas novas classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(include_top=False)\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo a camada densa para que seja adaptada para lidar com apenas 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = conv_base.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x) \n",
    "predictions = layers.Dense(3, activation='softmax')(x)\n",
    "model = Model(conv_base.input, predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, epochs=5, validation_steps=3, steps_per_epoch=3, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazenamento do modelo e carregamento do modelo pré-treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o modelo no formato HDf5\n",
    "model.save('modelos/model-frutas.h5')\n",
    "\n",
    "# arquitetura das camadas em JSSON e pesos treinados em HDF5\n",
    "model.save_weights('modelos/weights-frutas.h5')\n",
    "with open('modelos/architecture-frutas.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo no formato HDf5\n",
    "model = load_model('modelos/model-frutas.h5')\n",
    "\n",
    "# arquitetura das camadas em JSSON e pesos treinados em HDF5\n",
    "with open('modelos/architecture-frutas.json') as f:\n",
    "    model = model_from_json(f.read())\n",
    "    \n",
    "model.load_weights('modelos/weights-frutas.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Predição nas imagens de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagens_teste_path = [\"imagens-frutas/validation/Apple Braeburn/7_100.jpg\",\n",
    "                        \"imagens-frutas/validation/Avocado/49_100.jpg\",\n",
    "                        \"imagens-frutas/validation/Banana/12_100.jpg\",\n",
    "                        \"imagens/banana.jpeg\"]\n",
    "\n",
    "lista_imagem = []\n",
    "\n",
    "for imagem_path in imagens_teste_path:\n",
    "    imagem = cv2.imread(imagem_path)\n",
    "    imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "    imagem = cv2.resize(imagem, (224, 224))\n",
    "    lista_imagem.append(imagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_imagem_array = np.array(lista_imagem, dtype=\"float\")\n",
    "lista_imagem_array = preprocess_input(lista_imagem_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando as imagens de teste, como neste caso não usamos o gerador do Keras, precisamos ajustar manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict(lista_imagem_array)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i, imagem in enumerate(lista_imagem):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(imagem)\n",
    "    plt.title(\"{:.0f}% Apple, {:.0f}% Avocado, {:.0f}% Banana\".format(100*pred_probs[i,0], 100*pred_probs[i,1], 100*pred_probs[i,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classificador de Objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário baixar os pesos (modelo de deep-learning) neste link https://pjreddie.com/media/files/yolov3.weights e copiar para  pasta weights. O comando a seguir vai baixar o arquivo de pesos no diretório ```pesos```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar esse caminho por ser mais rápido o download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1paGkSIW97J-Cc1oLbuYdKkkK4zynq1SK -O pesos/yolov3.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights -P pesos/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurações do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações na rede neural YOLOv3\n",
    "config_file = 'cfg/yolov3.cfg'\n",
    "modelo_yolo = Darknet(config_file)\n",
    "\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'pesos/yolov3.weights'\n",
    "modelo_yolo.load_weights(weight_file)\n",
    "\n",
    "# Rótulos de classes\n",
    "class_names_file = 'data/coco.names'\n",
    "class_names = load_class_names(class_names_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topologia da rede neural da YOLOv3\n",
    "modelo_yolo.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho da imagem de entrada da rede: \" + str(modelo_yolo.width) + \" x \" + str(modelo_yolo.height) + \" pixels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando imagem para classificar\n",
    "imagem = cv2.imread(\"imagens/camara.jpg\")\n",
    "\n",
    "# Convertendo para o espaço de cores RGB\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatível com a primeira camada da rede neural  \n",
    "imagem_padronizada = cv2.resize(imagem, (modelo_yolo.width, modelo_yolo.height))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "# Exibição das imagens\n",
    "plt.title(\"Imagem Original\")\n",
    "plt.imshow(imagem)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Imagem Redimensionada\")\n",
    "plt.imshow(imagem_padronizada)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retângulo \n",
    "# de identificação de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do gráfico\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "\n",
    "# Deteteção de objetos na imagem\n",
    "boxes = detect_objects(modelo_yolo, imagem_padronizada, iou_thresh, nms_thresh)\n",
    "\n",
    "# Objetos encontrados e nível de confiança\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "# Desenho no gráfico com os regângulos e rótulos\n",
    "plot_boxes(imagem, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de todos os objetos identificados na imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(boxes, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
