{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rastreamento de objetos e machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rastreamento de cores, objetos e aplicaÃ§Ã£o de tÃ©cnicas de machine learning prÃ©-treinadas e desenvolvimento de algoritmos prÃ³prios.\n",
    "\n",
    "Alguns recursos e cÃ³digos foram adaptados deste [repositÃ³rio](https://github.com/udacity/CVND_Exercises/) do curso de VisÃ£o Computacional da Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AtenÃ§Ã£o: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versÃ£o local deste notebook, sem o sufixo ```-collab```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requerimentos\n",
    "\n",
    "### 1.1 Bibliotecas\n",
    "\n",
    "* OpenCV>=3.4.3\n",
    "* Pillow>= 7.0.0\n",
    "* Pytorch>=1.4.0\n",
    "* Numpy>=1.18.1\n",
    "\n",
    "### 1.2 Arquivos\n",
    "\n",
    "Baixe o repositÃ³rio do GitHub utilizando o comando abaixo. Em caso de atualizaÃ§Ã£o, utilize o comando para apagar o diretÃ³rio antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fiap-ml-visao-computacional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michelpf/fiap-ml-visao-computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora posicionar o diretÃ³rio do repositÃ³rio para a aula respectiva. Nesse caso envie o comando de mudanÃ§a de diretÃ³rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fiap-ml-visao-computacional/aula-5-machine-learning-aplicado/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImportaÃ§Ã£o das bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#ExibiÃ§Ã£o na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL\n",
    "\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reconhecimento Facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando 100 exemplos de faces, utilizando a cÃ¢mera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrator de faces\n",
    "def face_extractor(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "    imagem_gray = cv2.cvtColor(imagem,cv2.COLOR_RGB2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        face_recortada = imagem[y:y+h, x:x+w]\n",
    "\n",
    "    return face_recortada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraÃ§Ã£o de caracterÃ­sticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando imagems de exemplos para ser posteriormente treinado. Neste caso vamos adotar um tamanho de imagem para processamento de 200 x 200 (empÃ­rico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/camera_output_1.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"ExtraÃ§Ã£o de faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o diretÃ³rio ```imagens/faces/michel``` que contem imagens de treino de uma pessoa (eu ðŸ˜)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento do modelo\n",
    "\n",
    "Podemos testar os diversos tipos de classificadores, no entanto, o classificador LBPH tem o uso com melhor performance dentre o Eingenfaces e Fisherfaces.\n",
    "Neste caso como temos apenas uma Ãºnica pessoa, nosso dicionÃ¡rio de pessoas, ficou apenas com um Ãºnico registro. Em casos de multiclasses, ou seja, mais de uma pessoa, cada uma delas deve ter um *id* associado, que Ã© o valor de chave do dicionÃ¡rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando exemplos de arquivos previamente coletados\n",
    "faces_path = 'imagens/faces/michel/'\n",
    "lista_arquivos_imagens = [f for f in listdir(faces_path) if isfile(join(faces_path, f))]\n",
    "\n",
    "dados_treinamento, labels = [], []\n",
    "\n",
    "# Lendo as imagens e associando a um label\n",
    "for i, arquivos in enumerate(lista_arquivos_imagens):\n",
    "    imagem_path = faces_path + lista_arquivos_imagens[i]\n",
    "    imagem = cv2.imread(imagem_path, cv2.IMREAD_GRAYSCALE)\n",
    "    dados_treinamento.append(imagem)\n",
    "    labels.append(0)\n",
    "\n",
    "# Criando uma matriz da lista de labels\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "model.train(dados_treinamento, labels)\n",
    "\n",
    "print(\"Modelo treinado com sucesso.\")\n",
    "\n",
    "pessoas = {0: \"Michel\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InferÃªncia do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FunÃ§Ã£o para identificar o rosto e segmentar da imagme principal. TambÃ©m utilizaremos para desenhar um retÃ¢ngulo delimitador.\n",
    "Note que estamos normalizando a imagem (mesma escala, 200 x 200) que as imagens de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def face_detector(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "    imagem_gray = cv2.cvtColor(imagem,cv2.COLOR_RGB2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.1, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return imagem, [], 0, 0\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(imagem,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    \n",
    "    return imagem, roi, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo as imagems por meio da cÃ¢mera e fazendo a inferÃªncia on-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/analise-face-michel.PNG\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "imagem, face, x, y = face_detector(imagem)\n",
    "\n",
    "if face is not ():\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_RGB2GRAY)\n",
    "    predicao = model.predict(face)\n",
    "\n",
    "    if x > 0:\n",
    "        notificacao = \"Dist. \" + str(int(predicao[1])) + ' ' + pessoas[predicao[0]] \n",
    "        cv2.putText(imagem, notificacao, (x, y-20), cv2.FONT_HERSHEY_DUPLEX, 1, (255,120,150), 2)\n",
    "\n",
    "    if int(predicao[1]) < 40:\n",
    "        cv2.putText(imagem, \"Reconhecido com sucesso\", (x, y-50), cv2.FONT_HERSHEY_DUPLEX, 1, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(imagem, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"InferÃªncia do modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classificador de Objetos\n",
    "#### Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ã‰ necessÃ¡rio baixar os pesos (modelo de deep-learning) neste link https://pjreddie.com/media/files/yolov3.weights e copiar para  pasta weights. O comando a seguir vai baixar o arquivo de pesos no diretÃ³rio ```pesos```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights -P pesos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "# ConfiguraÃ§Ãµes na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Pesos prÃ©-treinados\n",
    "weight_file = 'pesos/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# RÃ³tulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topologia da rede neural da YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho da imagem de entrada da rede: \" + str(m.width) + \"x\" + str(m.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da figura\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregando imagem para classificar\n",
    "img_path = \"imagens/camara.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Convertendo para o espaÃ§o de cores RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatÃ­vel com a primeira camada da rede neural  \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# ExibiÃ§Ã£o das imagens\n",
    "plt.subplot(121)\n",
    "plt.title(\"Imagem Original\")\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Imagem Redimensionada\")\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retÃ¢ngulo \n",
    "# de identificaÃ§Ã£o de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do grÃ¡fico\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregar imagem para classificaÃ§Ã£o\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# ConversÃ£o para o espaÃ§o RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionamento para adatapÃ§Ã£o da primeira camada da rede neural \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# DeteteÃ§Ã£o de objetos na imagem\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Objetos encontrados e nÃ­vel de confianÃ§a\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "# Desenho no grÃ¡fico com os regÃ¢ngulos e rÃ³tulos\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(boxes, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
